% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/performCrossValidation-methods.R
\docType{methods}
\name{performCrossValidation,KernelMatrix-method}
\alias{CrossValidation}
\alias{cross.validation}
\alias{crossValidation}
\alias{performCrossValidation}
\alias{performCrossValidation,ExplicitRepresentation-method}
\alias{performCrossValidation,KernelMatrix-method}
\title{KeBABS Cross Validation}
\usage{
## kbsvm(......, cross=0, noCross=1, .....)

## please use kbsvm for cross validation and do not call the
## performCrossValidation method directly

\S4method{performCrossValidation}{ExplicitRepresentation}(object, x, y, sel,
  model, cross, noCross, groupBy, perfParameters, verbose)
}
\arguments{
  \item{object}{a kernel matrix or an explicit
  representation}

  \item{x}{an optional set of sequences}

  \item{y}{a response vector}

  \item{sel}{sample subset for which cross validation
  should be performed}

  \item{model}{KeBABS model}

  \item{cross}{an integer value K > 0 indicates that k-fold
  cross validation should be performed. A value -1 is used
  for Leave-One-Out (LOO) cross validation. (see above)
  Default=0}

  \item{noCross}{an integer value larger than 0 is used to
  specify the number of repetitions for cross validation.
  This parameter is only relevant if 'cross' is different
  from 0. Default=1}

  \item{groupBy}{allows a grouping of samples during cross
  validation. The parameter is only relevant when 'cross'
  is larger than 1. It is an integer vector or factor with
  the same length as the number of samples used for
  training and specifies for each sample to which group it
  belongs. Samples from the same group are never spread
  over more than one fold. Grouped cross validation can
  also be used in grid search for each grid point.
  Default=NULL}

  \item{perfParameters}{a character vector with one or
  several values from the set "ACC" , "BACC", "MCC", "AUC"
  and "ALL". "ACC" stands for accuracy, "BACC" for balanced
  accuracy, "MCC" for Matthews Correlation Coefficient,
  "AUC" for area under the ROC curve and "ALL" for all
  four. This parameter defines which performance parameters
  are collected in cross validation for display purpose.
  The summary values are computed as mean of the fold
  values. AUC computation from pooled decision values
  requires a calibrated classifier output and is currently
  not supported. Default=NULL}

  \item{verbose}{boolean value that indicates whether
  KeBABS should print additional messages showing the
  internal processing logic in a verbose manner. The
  default value depends on the R session verbosity option.
  Default=getOption("verbose")

  this parameter is not relevant for cross validation
  because the method \code{performCrossValidation} should
  not be called directly. Cross validation is performed
  with the method \code{\link{kbsvm}} and the parameters
  \code{cross} and \code{numCross} are described there}
}
\value{
cross validation stores the cross validation results in the
KeBABS model object returned by . They can be retrieved
with the accessor \code{\link{cvResult}} returned by
\code{\link{kbsvm}}.
}
\description{
Perform cross validation as k-fold cross validation,
Leave-One-Out cross validation(LOOCV) or grouped cross
validation (GCV).
}
\details{
Overview\cr

Cross validation (CV) provides an estimate for the
generalization performance of a model based on repeated
training on different subsets of the data and evaluating
the prediction performance on the remaining data not used
for training. Dependent on the strategy of splitting the
data different variants of cross validation exist. KeBABS
implements k-fold cross validation, Leave-One-Out cross
validation and Leave-Group-Out cross validation which is a
specific variant of k-fold cross validation. Cross
validation is invoked with \code{\link{kbsvm}} through
setting the parameters \code{cross} and \code{noCross}. It
can either be used for a given kernel and specific values
of the SVM hyperparameters to compute the cross validation
error of a single model or in conjuction with grid search
(see \link{gridSearch}) and model selection (see
\link{modelSelection}) to determine the performance of
multiple models.\cr\cr

k-fold Cross Validation and Leave-One-Out Cross
Validation(LOOCV)\cr

For k-fold cross validation the data is split into k
roughly equal sized subsets called folds. Samples are
assigned to the folds randomly. In k successive training
runs one of the folds is kept in round-robin manner for
predicting the performance while using the other k-1 folds
together as training data. Typical values for the number of
folds k are 5 or 10 dependent on the number of samples used
for CV. For LOOCV the fold size decreases to 1 and only a
single sample is kept as hold out fold for performance
prediction requiring the same number of training runs in
one cross validation run as the number of sequences used
for CV.\cr\cr

Grouped Cross Validation (GCV)\cr

For grouped cross validation samples are assigned to groups
by the user before running cross validation, e.g. via
clustering the sequences. The predefined group assignment
is passed to CV with the parameter \code{groupBy} in
\code{\link{kbsvm}}. GCV is a special version of k-fold
cross validation which respects group boundaries by
avoiding to distribute samples of one group over multiple
folds. In this way the group(s) in the test fold do not
occur during training and learning is forced to concentrate
on more complex features instead of the simple features
splitting the groups. For GCV the parameter cross must be
smaller than or equal to the number of groups.\cr\cr

Cross Validation Result\cr

The cross validation error, which is the average of the
predicition errors in all held out folds, is used as an
estimate for the generalization error of the model
assiciated with the cross validation run. For
classification the fraction of incorrectly classified
samples and for regression the mean squared error (MSE) is
used as prediction error. Multiple cross validation runs
can be performed through setting the parameter
\code{noCross}. The cross validation result can be
extracted from the model object returned by cross
validation with the \code{\link{cvResult}} accessor. It
contains the mean CV error over all runs, the CV errors of
the single runs and the CV error for each fold. The CV
result object can be plotted with the method
\code{\link{plot}} showing the variation of the CV error
for the different runs as barplot. With the parameter
\code{perfParameters} in \code{\link{kbsvm}} the accuracy,
the balanced accuracy and the Matthews correlation
coefficient can be requested as additional performance
parameters to be recorded in the CV result object which
might be of interest especially for unbalanced
datasets.\cr\cr
}
\examples{
## load transcription factor binding site data
data(TFBS)
enhancerFB
## select a few samples for training - here for demonstration purpose
## normally you would use 70 or 80\% of the samples for training and
## the rest for test
## train <- sample(1:length(enhancerFB), length(enhancerFB) * 0.7)
## test <- c(1:length(enhancerFB))[-train]
train <- sample(1:length(enhancerFB), 50)
## create a kernel object for the gappy pair kernel with normalization
gappy <- gappyPairKernel(k=1, m=4)
## show details of kernel object
gappy

## run cross validation with the kernel on C-svc in LiblineaR for cost=10
model <- kbsvm(x=enhancerFB[train], y=yFB[train], kernel=gappy,
               pkg="LiblineaR", svm="C-svc", cost=10, cross=3)

## show cross validation result
cvResult(model)

\dontrun{
## perform tive cross validation runs
model <- kbsvm(x=enhancerFB[train], y=yFB[train], kernel=gappy,
               pkg="LiblineaR", svm="C-svc", cost=10, cross=10, noCross=5)

## show cross validation result
cvResult(model)

## plot cross validation result
plot(cvResult(model))


## run Leave-One-Out cross validation
model <- kbsvm(x=enhancerFB[train], y=yFB[train], kernel=gappy,
               pkg="LiblineaR", svm="C-svc", cost=10, cross=-1)

## show cross validation result
cvResult(model)

## run gouped cross validation with full data
## on coiled coil dataset
##
## In this example the groups were determined through single linkage
## clustering of sequence similarities derived from ungapped heptad-specific
## pairwise alignment of the sequences. The variable {\\tt ccgroup} contains
## the pre-calculated group assignments for the individual sequences.
data(CCoil)
ccseq
head(yCC)
head(ccgroups)
gappyK1M6 <- gappyPairKernel(k=1, m=4)

## run k-fold CV without groups
model <- kbsvm(x=ccseq, y=as.numeric(yCC), kernel=gappyK1M6,
pkg="LiblineaR", svm="C-svc", cost=10, cross=3, noCross=2,
perfObjective="BACC",perfParameters=c("ACC", "BACC"))

## show result without groups
cvResult(model)

## run grouped CV
model <- kbsvm(x=ccseq, y=as.numeric(yCC), kernel=gappyK1M6,
pkg="LiblineaR", svm="C-svc", cost=10, cross=3,
noCross=2, groupBy=ccgroups, perfObjective="BACC",
perfParameters=c("ACC", "BACC"))

## show result with groups
cvResult(model)

## For grouped CV the samples in the held out fold are from a group which
## is not present in training on the other folds. The simimar CV error
## with and without groups shows that learning is not just assigning
## labels based on similarity within the groups but is focusing on features
## that are indicative for the class also in the CV without groups. For the
## GCV no information about group membership for the samples in the held
## out fold is present in the model. This example should show how GCV
## is performed. Because of package size limitations no specific dataset is
## available in this package where GCV is necessary.
}
}
\author{
Johannes Palme <kebabs@bioinf.jku.at>
}
\references{
\url{http://www.bioinf.jku.at/software/kebabs}\cr\cr J.
Palme, S. Hochreiter, and U. Bodenhofer (2015) KeBABS: an R
package for kernel-based analysis of biological sequences.
\emph{Bioinformatics} (accepted). DOI:
\href{http://dx.doi.org/10.1093/bioinformatics/btv176}{10.1093/bioinformatics/btv176}.
}
\seealso{
\code{\link{kbsvm}}, \code{\link{cvResult}},
\code{\link{plot}}
}
\keyword{cross}
\keyword{grid}
\keyword{kbsvm}
\keyword{methods}
\keyword{model}
\keyword{search}
\keyword{selection}
\keyword{validation}

